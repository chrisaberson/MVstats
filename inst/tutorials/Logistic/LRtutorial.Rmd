---
title: "Logistic Regression Tutorial"
author: "Chris Aberson"
output:
  learnr::tutorial:
    progressive: false
runtime: shiny_prerendered

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(learnr)
hint_text <- function(text, text_color = "#E69F00"){
  hint <- paste("<font color='", text_color, "'>", text, "</font>", sep = "")
  return(hint)
}

```

# Logistic Regresison

## Overview

This tutorial focuses on learning about logistic regression. I accomplish this with a combination of videos, text, and exercises. The videos are as before but now you can run example code and do exercises right in the tutorial. I can even add questions so that you can make sure you understand concepts before moving on. 

The powerpoint slides for the presentation in the videos are on Canvas if you want a copy. 

## Packages 

This tutorial uses the following packages:

* `popbio` for cool graphing function
* `lmtest` for likelihood ratio tests
* `learnr` and `shiny` and `rmarkdown` for aspects of the tutorial


These packages are automatically loaded within this tutorial. If you are working outside of this tutorial (i.e. in **RStudio**) then you need to make sure that the package has been installed by executing `install.packages("package_name")`, where *package_name* is the name of the package. If the package is already installed, then you need to reference it in your current session by executing `library(package_name)`, where *package_name* is the name of the package.

## Data

All data files exist within this package, so we can simply call them without reference to a file location. 

*logistic2* These data represent 164 women. The data examine compliance with mammogram screeing recommendations (i.e., did they get a mammogram?). The variables are **comply** coded as 0 = No, 1 = Yes (this is the outcome variables), **phyrec** also coded as 0 = No, 1 = Yes, representing whether their doctor recommended them for screening (i.e., did their doctor tell them to schedule a screening), **knowledge** a continuously scale measures of knowledge about mammograms, and two variables measuring perceptions about mammograms - **benefits** and **barriers**. 

*admit* These data examine Ph.D. program admissions. The variable **admit** is coded as 0 = No, 1 = Yes, representing whether or not the student was admitted (this is the outcome variable). The predictors are **gre**, **gpa**, and **rank** (ranking of the candidate  from letter writers).

Both files are on the class' Google Share drive if you want to work with them outside of the tutorial. They are pre-loaded in this tutorial so you will not need to load the data as you work. 

## Video 1: Logistic Regression Basics and Terms

![](https://youtu.be/xw2NyvAhMp0)

## Quiz 1: Basics

```{r logquiz_dv}
learnr::quiz(
  learnr::question("When do we use Logistic Regression",
    learnr::answer("When we have dichotomous outcome variables", correct = TRUE),
    learnr::answer("When linear regression assumptions fail"),
    learnr::answer("When we have categorical predictors"),
    learnr::answer("When we have dichotomous predictors"),
    correct = "Correct!",
    incorrect = "Sorry, that's incorrect. Try again.",
    random_answer_order = TRUE,
    allow_retry = T
  ),
  learnr::question("Why do we use Logistic instead of regular linear regression?",
    learnr::answer("Because we are not examining linear relationships",correct=TRUE),
    learnr::answer("Because dichotomous predictors can't be normalized"),
    learnr::answer("To handle violations of linear regression assumptions"),
    correct = "Correct",
    random_answer_order = TRUE,
    incorrect = "Sorry, that's incorrect. Try again.",
    allow_retry = T
  ),
  learnr::question("For the *log odds ratio* a negative relationship is represented by ... ",
    learnr::answer("A negative coefficient", correct = TRUE),
    learnr::answer("A positive coefficient"),
    learnr::answer("Values below 1"),
    learnr::answer("Values above 1"),
    correct = "Correct!",
    incorrect = "Incorrect. Please try again.",
    random_answer_order = TRUE,
    allow_retry = T
  ),
  learnr::question("For the *odds ratio* a negative relationship is represented by ... ",
    learnr::answer("Values below 1", correct = TRUE),
    learnr::answer("A negative coefficient"),
    learnr::answer("A positive coefficient"),
    learnr::answer("Values above 1"),
    correct = "Correct!",
    incorrect = "Incorrect. Please try again.",
    random_answer_order = TRUE,
    allow_retry = T
  )
)

``` 

## Examining our data

```{r echo = F, eval = T}
logistic2<-MVstats::logistic2
admit<-MVstats::admit
```


```{r echo = T, eval = F}
str(logistic2)
```   

### Code with options to run and solution

Paste the code into the run window below. The str command tells us what kind of data we have. Our primary interest here are variables that are categorical. They have to be explicitly recoded as factors with each level being given a name. 

```{r ex0, exercise = TRUE, exercise.lines = 1}

```

Of note here are that two of the variables are numeric but only have scores of 0 and 1. Those variables are **physrec** and **comply**. Assuming that 0 mean "No" and 1 means "Yes" we will need to define those variables as Factors and assign labels to the different factor levels. I added two variables noted as **phyrec_F** and **comply_F** that represent these are factors. 

## Coding the Factors

The code below demonstrates one way to recode a variable. The summary command provides a way to verify that the coding was done correctly. 

```{r echo = T, eval = T}
logistic2$physrec_F<-factor(logistic2$physrec,
                          levels = c(0:1),
                          labels = c("No", "Yes"))
summary(logistic2$physrec_F)

```

### Exercise 1: Variable Coding

Use the datafile *admit* (again note that these data are already loaded in your workspace). In this datafile , the outcome variable is called **admit** (yes both the data file and variable have the same name). **admit** will need to be recoded (0 = No, 1 = Yes). Run the summary on the new variable (call the new variable admit_F)

```{r ex1, exercise = TRUE, exercise.lines = 5}

```

```{r ex1-solution}
admit$admit_F<-factor(admit$admit,
                          levels = c(0:1),
                          labels = c("No", "Yes"))
summary(admit$admit_F)
```

Make sure you have run this analysis. Your code later in the tutorial will not work unless you complete this task. 

## Video 2: Try it as Linear Regression 

![](https://youtu.be/v-EeunqPUok)


## With Linear Regression Examples (a.k.a., why we can't use linear regression)

In this section, you will run the code from the video and then do an exercise where you adapt that code to the admit data. 

Note that this section is to demonstrate why we cannot use linear regression with dichotomous outcome (dependent) variables. 

In the video, we started with using linear regression using this code. Note that I am using the variable that was simply numeric here as lm will not accept factors as dependent variables. 

```{r echo = T, eval =T}
reg<-lm(comply~physrec,data=logistic2)
summary(reg)
plot(reg)
```


## Bonus: Jittering the plot!

The code below pulls the predicted values from our analysis into the data set (line 1), calculates the residual (line 2), plots them with a jitter (line 3), and then add the line (line 4). 

Jittering is useful when we have categorical values on a scatter plot as these values "pile up" - that is there might be 20 scores in the same place on the graph but we can't really see them because they are all on top of each other. The jitter function adds random noise to the values so we can better see the patterns. 

```{r echo = T, eval = T}
reg<-lm(comply~physrec,data=logistic2)
logistic2$predicted<-round((reg$fitted.values),2)
logistic2$residuals<-logistic2$comply-logistic2$predicted
plot(logistic2$residuals ~ jitter(logistic2$predicted, 1), pch = 1)
abline(lm(logistic2$residuals~logistic2$predicted))
```

## Exercise 2: Running linear regression on the admit data

For the *admit* data predict admit from GRE scores (name the model object ex2), summarize the model, and add plot the results (first using the regular plot command). How do the residuals look? (Note: We don't need to jitter here as GRE is a continuous predictor). The variable names are **admit** and **gre**.

```{r ex2, exercise = TRUE, exercise.lines = 3}

```

```{r ex2-solution}
ex2<-lm(admit~gre, data = admit)
summary(ex2)
plot(ex2)
```


## Video #3: Running Logistic Regression with 1 Predictor 

![](https://youtu.be/o2uv83uD7-c)


## Quiz 2 First Steps

```{r logquiz2}
learnr::quiz(
  learnr::question("The first step in running logistic regression is",
    learnr::answer("Coding categorical variables as factors", correct = TRUE),
    learnr::answer("Graphing the residuals"),
    learnr::answer("Running linear regression"),
    learnr::answer("Converting from log odds to odds"),
    correct = "Correct!",
    incorrect = "Sorry, that's incorrect. zzzt - the experiment requires that you continue.",
    random_answer_order = TRUE,
    allow_retry = T
  )
)

``` 


## Examples from the video

In the video, we saw the following logisitic regression. Recall that we needed to code the variables to be factors before running logistic. 

Model1<-glm(comply~physrec, data=logistic2, family = binomial)
summary(Model1)

```{r echo = T, eval = F}
Model1<-glm(comply_F~physrec_F, data=logistic2, family = binomial)
summary(Model1)
``` 
Use the window below to run the code to reproduce results from the video. 

```{r ex02, exercise = TRUE, exercise.lines = 2}

```

Next, convert the log odds ratios given in the summary to odds ratios.

```{r echo = T, eval = F}
exp(Model1$coefficients)
exp(confint(Model1))
``` 

Use the window below to run the code to reproduce results from the video. 

```{r ex03, exercise = TRUE, exercise.lines = 2}

```

## Exercise 3: Running Logistic Regression with One Predictor

Using the admit data from earlier, run a logistic regression predicting admission (admit( from GRE scores (GRE). Name your model ex3. Be sure to convert to odds ratios and get confidence intervals as well. 

```{r ex3, exercise = TRUE, exercise.lines = 4}

```

```{r ex3-solution}
ex3<-glm(admit_F~gre, data=admit, family = binomial)
summary(ex3)
exp(ex3$coefficients)
exp(confint(ex3))
```

### Quiz 3: Interpret Results

```{r logquiz3}
learnr::quiz(
  learnr::question("Based on the results of your analysis",
    learnr::answer("Higher GRE scores relate to a greater likelihood of admission", correct = TRUE),
    learnr::answer("GRE scores are not related to admission"),
    learnr::answer("Lower GRE scores relate to a greater likelihood of admission"),
    correct = "Correct!",
    incorrect = "Sorry, that's incorrect.",
    random_answer_order = TRUE,
    allow_retry = T
  )
)

``` 

## Model Fit

Just like in regression, we get a statistic that addresses whether or not the model significantly predicts outcomes. Those are not particularly useful with just one predictor but we will examine those now so we can run use those values later for models with multiple predictors. We will also examine how well our models predict outcomes (something that is useful regardless of the number of predictors).

The basic process here is to take the difference between the null deviance model and the residual deviance model. 


```{r}
Model1<-glm(comply_F~physrec_F, data=logistic2, family = binomial)
summary(Model1)
```

The null model reflects a model with no predictors. This is sometimes called a "intercept-only" model. It is how well we can predict **comply** without any predictors. One way to think about this is by asking "how many people could we accurately classify" simply based on descriptive statistics. Let's start by examining the summary below. 

```{r}
summary(logistic2$comply_F)
```

Based on these numbers, we could correctly classify 88 people out of the total of 164 (88+76) by simply predicting everyone was in the "No" category. 

Comparing the null model and out prediction model (the one where we used physrec as a predictor) is accomplished with the code below. For some reason, the logistic regression output does not provide that test but the code below accomplishes. 

```{r}
modelChi<-Model1$null.deviance-Model1$deviance
modelChi
chidf<-Model1$df.null-Model1$df.residual
chidf
chisq.prob<-1-pchisq(modelChi, chidf)
chisq.prob
```

Here we get a chi-square value of 34.6 based on 1 df (each predictor uses a df) with *p* < .001. 

### Exercise 4: Model Fit with Chi-Square

Using the admit data from earlier, run a logistic regression predicting admission (admit from GRE scores (GRE). Name your model ex4 this time. Derive the Chi-square, df, and probability. 

```{r ex4, exercise = TRUE, exercise.lines = 7}

```

```{r ex4-solution}
ex4<-glm(admit_F~gre, data=admit, family = binomial)
modelChi<-ex4$null.deviance-ex4$deviance
modelChi
chidf<-ex4$df.null-ex4$df.residual
chidf
chisq.prob<-1-pchisq(modelChi, chidf)
chisq.prob
```

## Digging deeper on model fit: Null model

We can examine the null by running a logistic regression without any predictors. To do that the code below uses ~1. You can use this is in any prediction model to run a null (a.k.a., intercept only) model. 

```{r}
null<-glm(comply_F~1, data=logistic2, family = binomial)
summary(null)
```

Note that these deviance statistics are exactly the same as we saw in Model 1 under Null Deviance. 

```{r}
summary(Model1)
```

## More Model Fit: Predicted outcome vs. actual outcome

Another way to evaluate model fit is to compare actual outcomes to predicted outcomes. The code below accomplishes this. Technically, we are taking the fitted (predicted) probabilty of group membership and assigning people to groups based on those predicted probabilities. 

```{r}
table(null$y,fitted(null)>.5)
```

Across the top is the prediction. Here, with no predictors (only the intercept) we predict everyone to be "False" (a No response on the outcome variable). On the left hand side, the 0 and 1 reflect the actual scores, again showing that we have 88 Nos and 76 Yeses. Even without preditors, we still could get about 54% correct. 

Now, examining the model that included predictors. 

```{r}
table(Model1$y,fitted(Model1)>.5)
```

Here, we see we are now predicting both True (Yes) and False (No) outcomes. We see that of the 88 people who did not get screened, we predicted 44 correctly as False but also 44 incorrectly as True - so we have a 50% accuracy rate. Among those that did get screened (the 1s), we got 69 correct but only 7 wrong (91% correct!). Overall we have 113 correct responses (44+69) and 51 incorrect (7+44). In total, we have 69% right compared to 54% without any predictors. 

It is always important to interpret this statistic in comparison to the null model. You may be able to correctly classify 70% in a model with predictors but that means very different things if the null model correctly classifies 68% vs. 50%. If the null classifies 68% correctly, our new model (with predictors) isn't much of an improvement whereas if the null classifies 50% correctly, our new model substantially improves predction. 

### Exercise 5: Classification Tables

Build a classification table for the admit data. Use the model built in the previous exercise called ex4 to derive the table. 

```{r ex5, exercise = TRUE, exercise.lines = 1}

```

```{r ex5-solution}
table(ex4$y,fitted(ex4)>.5)
```

Quiz 4: Interpretation

```{r logquiz4}
learnr::quiz(
  learnr::question("Based on the classification table, which statement is true",
    learnr::answer("Including GRE as a predictor did not increase the number of correctly  classified cases", correct = TRUE),
    learnr::answer("GRE scores improved prediction"),
    learnr::answer("GRE scores made prediction worse"),
    correct = "Correct!",
    incorrect = "Sorry, that's incorrect.",
    random_answer_order = TRUE,
    allow_retry = T
  )
)

``` 

Note that in the prediction model we did see that GRE scores were significantly related to admission but the classification statistics tell us that we still didn't improve prediction accuracy over the null. 

```{r}
summary(glm(admit_F~gre, data=admit, family = binomial))
```

## Video #4: Logistic Regression with Multiple Predictors

![](https://youtu.be/IuCv-E-KNPw)


### Quiz 5: Interactions?


```{r logquiz5}
learnr::quiz(
  learnr::question("Logistic regression cannot handle interactions: ",
    learnr::answer("False", correct = TRUE),
    learnr::answer("True"),
    correct = "Correct!",
    incorrect = "Sorry, that's incorrect.",
    random_answer_order = TRUE,
    allow_retry = T
  ),
  learnr::question("Centering is not necessary in logistic regression: ",
    learnr::answer("False", correct = TRUE),
    learnr::answer("True"),
    correct = "Correct!",
    incorrect = "Sorry, that's incorrect.",
    random_answer_order = TRUE,
    allow_retry = T
  )
)

``` 


## Logistic Regression with Multiple Predictors

This analysis appeared in the video. 

```{r}
Model4<-glm(comply~physrec+knowledg+benefits+barriers, data=logistic2, family = binomial())
summary(Model4)
exp(Model4$coefficients)
exp(confint(Model4))
modelChi<-Model4$null.deviance-Model4$deviance
modelChi
chidf<-Model4$df.null-Model4$df.residual
chidf
chisq.prob<-1-pchisq(modelChi, chidf)
chisq.prob
```
To interpret this model, start with the model chi-square values. We have $\chi^2$(1) = 58.8, *p* < .001. This tells us the four variables together significantly predict compliance. 

From there, move to the specific predictors. My preference is to use *exp(b)*, the odds ratio. Below I interpret each using their 95% ci's but you can also do this with the probability. Recall that if the CI does not include 1, that we reject the null (a value of 1 means the odds of admission were completely unrelate to the outcome). 

Physician's recommendation related to greater compliance with screeing, *exp(b)* = 6.31, *95% CI* [2.53,17.57]. Similarly, those who perceived more benefits from mammography were more likely to comply, *exp(b)* = 1.72, *95% CI* [1.08,2.82]. Perceived barriers related to less compliance, *exp(b)* = 0.56, *95% CI* [0.40,0.77], Knowledge was unrelated to compliance, *exp(b)* = 0.92, *95% CI* [0.11,7.69] 

Turning to classification, the table below shows us jumping to 69% correctly classified (113 correct out of 164 total).

```{r}
table(Model4$y,fitted(Model4)>.5)
```

### Exercise 6: Running logistic with mutliple predictors

Run logistic regression using the admit data. Predict admit_F (the variable) from gre, gpa, and rank. Call your model ex6. 

```{r ex6, exercise = TRUE, exercise.lines = 7}

```

```{r ex6-solution}
ex6<-glm(admit_F~gre+gpa+rank, data=admit, family = binomial)
summary(ex6)
exp(ex6$coefficients)
exp(confint(ex6))
modelChi<-ex6$null.deviance-ex6$deviance
modelChi
chidf<-ex6$df.null-ex4$df.residual
chidf
chisq.prob<-1-pchisq(modelChi, chidf)
chisq.prob
```

### Quiz 6 Interpreting Multiple Predictor Models

```{r logquiz6}
learnr::quiz(
  learnr::question("What statistics address whether the model predicted admission?",
    learnr::answer("Chi-square", correct = TRUE),
    learnr::answer("Log odds ratios (coefficients)"),
    learnr::answer("Odds ratios (exponentiated coefficients)"),
    correct = "Correct!",
    incorrect = "Sorry, that's incorrect.",
    random_answer_order = TRUE,
    allow_retry = T
  ),
  learnr::question("Was the model statistically significant?",
    learnr::answer("Yes", correct = TRUE),
    learnr::answer("No"),
    correct = "Correct!",
    incorrect = "Sorry, that's incorrect.",
    random_answer_order = TRUE,
    allow_retry = T
  ),
  learnr::question("Which variables uniquely predicted admission?",
    learnr::answer("All of them", correct = TRUE),
    learnr::answer("Just GRE"),
    learnr::answer("Just GPA"),
    learnr::answer("Just Rank"),
    learnr::answer("Just GRE and GPA"),
    correct = "Correct!",
    incorrect = "Sorry, that's incorrect.",
    random_answer_order = TRUE,
    allow_retry = T
  ),
  learnr::question("Which is the best conclusion regarding GRE scores",
    learnr::answer("Higher GRE scores related to a greater chance of admission", correct = TRUE),
    learnr::answer("Lower GRE scores related to a greater chance of admission"),
    learnr::answer("GRE scores were unrelated to admission decisions"),
    correct = "Correct!",
    incorrect = "Sorry, that's incorrect.",
    random_answer_order = TRUE,
    allow_retry = T
  ),
  learnr::question("Which is the best conclusion regarding GPA",
    learnr::answer("Higher GPA related to a greater chance of admission", correct = TRUE),
    learnr::answer("Lower GPA related to a greater chance of admission"),
    learnr::answer("GPA was unrelated to admission decisions"),
    correct = "Correct!",
    incorrect = "Sorry, that's incorrect.",
    random_answer_order = TRUE,
    allow_retry = T
  ),
  learnr::question("Which is the best conclusion regarding rank (recall that rank is sort of reversed, a rank of 1 is highest and a rank of 4 is lowest",
    learnr::answer("More favorable rankings (e.g., a 1 or 2) related to a greater chance of admission", correct = TRUE),
    learnr::answer("Less favorable rankings (e.g., a 3 or 4) related to a greater chance of admission"),
    learnr::answer("Rankings were unrelated to admission decisions"),
    correct = "Correct!",
    incorrect = "Sorry, that's incorrect.",
    random_answer_order = TRUE,
    allow_retry = T
  )
  
)

``` 

## Exercise 7: Classification for mutliple predictors

Recall the code for classification from earlier. 

```{r}
table(Model4$y,fitted(Model4)>.5)
```

Adapt this code to the model in the previous exercise to build a classification table for the *admit* data. You can use the analysis you ran called ex6.

```{r ex7, exercise = TRUE, exercise.lines = 7}

```

```{r ex7-solution}
table(ex6$y,fitted(ex6)>.5)
```

Quiz 7: Classification 

```{r logquiz7}
learnr::quiz(
  learnr::question("What percent of the total number of no outcomes (coded as 0) were correctly classified (as False)? ",
    learnr::answer("Over 90%", correct = TRUE),
    learnr::answer("Under 10%"),
    learnr::answer("Around 70"),
    correct = "Correct! There are 273 total 0 responses (253+20). 253 of those were correctly classified",
    incorrect = "Sorry, that's incorrect.",
    random_answer_order = TRUE,
    allow_retry = T
  ),
learnr::question("What percent of the total number of yes outcomes (coded as 1) were correctly classified (as True)? ",
    learnr::answer("Around 20%", correct = TRUE),
    learnr::answer("Over 75%"),
    learnr::answer("Around 70"),
    correct = "Correct! There are 127 total 1 responses (98+29). Only 29 of those were correctly classified",
    incorrect = "Sorry, that's incorrect.",
    random_answer_order = TRUE,
    allow_retry = T
  ),
learnr::question("What percent of the total number  outcomes were correctly classified? ",
    learnr::answer("Around 70%", correct = TRUE),
    learnr::answer("Under 30%"),
    learnr::answer("Nearly 100%"),
    correct = "Correct! There are 282 correct classifications (253+29) out of a total of 400 cases.",
    incorrect = "Sorry, that's incorrect.",
    random_answer_order = TRUE,
    allow_retry = T
  )
)
``` 

## Video #5: Logistic Regression Other Useful Statistics

![](https://youtu.be/z55gaV3CjzQ)

## Likelihood ratio $\chi^2$

The LR $\chi^2$ is a more stable estimate of statistical significance than are the z-tests provided in R. Although not really the case in our examples, those values can tend to go crazy with smaller samples and complex models. 

Running the LR $\chi^2$ requires that we first build a model with all of our predictors. Then we build models with all but one of the predictors included. In the example from the video with four predictors this meant that we created four additional models, in each one we excluded one (and only one) of our variables. Model 4 is the full model (with all four predictors). Model.phys has everything but physician's recommendation. Model.know has everything but knowledge, etc. 

```{r}
Model4<-glm(comply~physrec+knowledg+benefits+barriers, data=logistic2, family = binomial())
Model.phys<-glm(comply~knowledg+benefits+barriers, data=logistic2, family = binomial())
Model.know<-glm(comply~physrec+benefits+barriers, data=logistic2, family = binomial())
Model.benef<-glm(comply~physrec+knowledg+barriers, data=logistic2, family = binomial())
Model.barr<-glm(comply~physrec+knowledg+benefits, data=logistic2, family = binomial())
```

Next we use the `lmtest` package to build model comparisons. 

```{r}
lmtest::lrtest(Model4,Model.phys)
lmtest::lrtest(Model4,Model.know)
lmtest::lrtest(Model4,Model.benef)
lmtest::lrtest(Model4,Model.barr)
```

The result for the first comparison (Model4 vs. Model.phys) is the LR $\chi^2$ for phys and so on down the line. 

### Exercise 8 - Likelihood Ratio $\chi^2$ 

Build your models here. Remember you will need one full model and then several (depending on the number of predictors) that exclude a single variable. 

```{r ex8, exercise = TRUE, exercise.lines = 4}

```

```{r ex8-solution}
ModelFull<-glm(admit~gre+gpa+rank, data=admit, family = binomial)
Model.gre<-glm(admit~gpa+rank, data=admit, family = binomial)
Model.gpa<-glm(admit~gre+rank, data=admit, family = binomial)
Model.rank<-glm(admit~gre+gpa, data=admit, family = binomial)
```

Once you've built the models, compare them using lrtest from the `lmtest` package. This should yield three tests. .

```{r ex8b, exercise = TRUE, exercise.lines = 3}

```

```{r ex8b-solution}
lmtest::lrtest(ModelFull,Model.gre)
lmtest::lrtest(ModelFull,Model.gpa)
lmtest::lrtest(ModelFull,Model.rank)

```

## *$R^2$* Analogues (*$R^2_L$*) 

The final value we examined in the video was *$R^2_L$*. The code below calculated the statistic from our final model. 

```{r}
R2L<-(Model4$null.deviance-Model4$deviance) / Model4$null.deviance
R2L
```


### Exercise 9 Computing *$R^2_L$*

Adapt the code above to the *admit* data. Be sure to use your final model. 

```{r ex9, exercise = TRUE, exercise.lines = 2}

```

```{r ex9-solution}
R2L<-(ModelFull$null.deviance-ModelFull$deviance) / ModelFull$null.deviance
R2L
```

Congratulations! You've reached the end of the tutorial. 
