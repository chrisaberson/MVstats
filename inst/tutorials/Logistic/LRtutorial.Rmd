---
title: "Logistic Regression Tutorial"
author: "Chris Aberson"
output:
  learnr::tutorial:
    progressive: false
runtime: shiny_prerendered

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(learnr)
hint_text <- function(text, text_color = "#E69F00"){
  hint <- paste("<font color='", text_color, "'>", text, "</font>", sep = "")
  return(hint)
}

```

# Logistic Regresison

## Overview

This tutorial focuses on learning about logistic regression. I accomplish this with a combination of videos, text, and exercises. The videos are as before but now you can run example code and do exercises right in the tutorial. I can even add questions so that you can make sure you understand concepts before moving on. 

The powerpoint slides for the presentation in the videos are on Canvas if you want a copy. 

## Packages 

This tutorial uses the following packages:

* `popbio` for cool graphing function
* `lmtest` for likelihood ratio tests
* `learnr` and `shiny` and `rmarkdown` for aspects of the tutorial


These packages are automatically loaded within this tutorial. If you are working outside of this tutorial (i.e. in **RStudio**) then you need to make sure that the package has been installed by executing `install.packages("package_name")`, where *package_name* is the name of the package. If the package is already installed, then you need to reference it in your current session by executing `library(package_name)`, where *package_name* is the name of the package.

## Data

All data files exist within this package, so we can simply call them without reference to a file location. 

*logistic2* These data represent 164 women. The data examine compliance with mammogram screeing recommendations (i.e., did they get a mammogram?). The variables are **comply** coded as 0 = No, 1 = Yes (this is the outcome variables), **phyrec** also coded as 0 = No, 1 = Yes, representing whether their doctor recommended them for screening (i.e., did their doctor tell them to schedule a screening), **knowledge** a continuously scale measures of knowledge about mammograms, and two variables measuring perceptions about mammograms - **benefits** and **barriers**. 

*admit* These data examine Ph.D. program admissions. The variable **admit** is coded as 0 = No, 1 = Yes, representing whether or not the student was admitted (this is the outcome variable). The predictors are **gre**, **gpa**, and **rank** (ranking of the candidate  from letter writers).

Both files are on the class' Google Share drive if you want to work with them outside of the tutorial. They are pre-loaded in this tutorial so you will not need to load the data as you work. 

## Logistic Regression: Basics and Terms Video

![](https://youtu.be/AJsKUo-XwZQ) Video 1

## Quiz

```{r logquiz_dv}
learnr::quiz(
  learnr::question("When do we use Logistic Regression",
    learnr::answer("When we have dichotomous outcome variables", correct = TRUE),
    learnr::answer("When linear regression assumptions fail"),
    learnr::answer("When we have categorical predictors"),
    learnr::answer("When we have dichotomous predictors"),
    correct = "Correct!",
    incorrect = "Sorry, that's incorrect. Try again.",
    random_answer_order = TRUE,
    allow_retry = T
  ),
  learnr::question("Why do we use Logistic instead of regular linear regression?",
    learnr::answer("Because we are not examining linear relationships",correct=TRUE),
    learnr::answer("Because dichotomous predictors can't be normalized"),
    learnr::answer("To handle violations of linear regression assumptions"),
    correct = "Correct",
    random_answer_order = TRUE,
    incorrect = "Sorry, that's incorrect. Try again.",
    allow_retry = T
  ),
  learnr::question("For the *log odds ratio* a negative relationship is represented by ... ",
    learnr::answer("A negative coefficient", correct = TRUE),
    learnr::answer("A positive coefficient"),
    learnr::answer("Values below 1"),
    learnr::answer("Values above 1"),
    correct = "Correct!",
    incorrect = "Incorrect. Please try again.",
    random_answer_order = TRUE,
    allow_retry = T
  ),
  learnr::question("For the *odds ratio* a negative relationship is represented by ... ",
    learnr::answer("Values below 1", correct = TRUE),
    learnr::answer("A negative coefficient"),
    learnr::answer("A positive coefficient"),
    learnr::answer("Values above 1"),
    correct = "Correct!",
    incorrect = "Incorrect. Please try again.",
    random_answer_order = TRUE,
    allow_retry = T
  )
)

``` 

## Examining our data

```{r echo = T, eval = F}
str(logistic2)
```   

### Code with options to run and solution

Paste the code into the run window below. The str command tells us what kind of data we have. Our primary interest here are variables that are categorical. They have to be explicitly recoded as factors with each level being given a name. 

```{r ex1, exercise = TRUE, exercise.lines = 1}

```

Of note here are that two of the variables are numeric but only have scores of 0 and 1. Those variables are **physrec** and **comply**. Assuming that 0 mean "No" and 1 means "Yes" we will need to define those variables as Factors and assign labels to the different factor levels. I added two variables noted as **phyrec_F** and **comply_F** that represent these are factors. 

## Coding the Factors

The code below demonstrates one way to recode a variable. The summary command provides a way to verify that the coding was done correctly. 

```{r echo = T, eval = T}
logistic2$physrec_F<-factor(logistic2$physrec,
                          levels = c(0:1),
                          labels = c("No", "Yes"))
summary(logistic2$physrec_F)
```

### Exercise 1: Variable Coding

Use the datafile *admit* (again note that these data are already loaded in your workspace). In this datafile , the outcome variable is called **admit** (yes both the data file and variable have the same name). **admit** will need to be recoded (0 = No, 1 = Yes). Run the summary on the new variable (call the new variable admit_F)

```{r ex4, exercise = TRUE, exercise.lines = 5}

```

```{r ex4-solution}
admit$admit_F<-factor(admit$admit,
                          levels = c(0:1),
                          labels = c("No", "Yes"))
summary(admit$admit_F)
```

## Examine Relationship Using Linear Regression 

![](https://youtu.be/AJsKUo-XwZQ) Video 2


## With Linear Regression Examples (a.k.a., why we can't use linear regression)

In this section, you will run the code from the video and then do an exercise where you adapt that code to the admit data. 

Note that this section is to demonstrate why we cannot use linear regression with dichotomous outcome (dependent) variables. 

In the video, we started with using linear regression using this code. Note that I am using the variable that was simply numeric here as lm will not accept factors as dependent variables. 

```{r echo = T, eval = F}
reg<-lm(comply~physrec,data=logistic2)
summary(reg)
plot(reg)
```

Paste the code into the box below to run the analyses. 

```{r ex5, exercise = TRUE, exercise.lines = 3}

```

```{r echo = F, eval = F}
reg<-lm(comply~physrec,data=logistic2)
summary(reg)
plot(reg)
```

## Bonus: Jittering the plot!

The code below pulls the predicted values from our analysis into the data set (line 1), calculates the residual (line 2), plots them with a jitter (line 3), and then add the line (line 4). 

Jittering is useful when we have categorical values on a scatter plot as these values "pile up" - that is there might be 20 scores in the same place on the graph but we can't really see them because they are all on top of each other. The jitter function adds random noise to the values so we can better see the patterns. 

```{r echo = T, eval = T}
reg<-lm(comply~physrec,data=logistic2)
logistic2$predicted<-round((reg$fitted.values),2)
logistic2$residuals<-logistic2$comply-logistic2$predicted
plot(logistic2$residuals ~ jitter(logistic2$predicted, 1), pch = 1)
abline(lm(logistic2$residuals~logistic2$predicted))
```

## Exercise 2: Running linear regression on the admit data

For the *admit* data predict admit from GRE scores (name the model object ex7), summarize the model, and add plot the results (first using the regular plot command). How do the residuals look? (Note: We don't need to jitter here as GRE is a continuous predictor). The variable names are **admit** and **gre**.

```{r ex7, exercise = TRUE, exercise.lines = 3}

```

```{r ex7-solution}
ex7<-lm(admit~gre, data = admit)
summary(ex7)
plot(ex7)
```


## Running Logistic Regression: 1 Predictor 

![](https://youtu.be/AJsKUo-XwZQ) Video 3


## Quiz

```{r logquiz2}
learnr::quiz(
  learnr::question("The first step in running logistic regression is",
    learnr::answer("Coding categorical variables as factors", correct = TRUE),
    learnr::answer("Graphing the residuals"),
    learnr::answer("Running linear regression"),
    learnr::answer("Converting from log odds to odds"),
    correct = "Correct!",
    incorrect = "Sorry, that's incorrect. zzzt - the experiment requires that you continue.",
    random_answer_order = TRUE,
    allow_retry = T
  )
)

``` 


## Examples from the video

In the video, we saw the following logisitic regression. Recall that we needed to code the variables to be factors before running logistic. 

Model1<-glm(comply~physrec, data=logistic2, family = binomial)
summary(Model1)

```{r echo = T, eval = F}
Model1<-glm(comply_F~physrec_F, data=logistic2, family = binomial)
summary(Model1)
``` 
Use the window below to run the code to reproduce results from the video. 

```{r ex8, exercise = TRUE, exercise.lines = 2}

```

Next, convert the log odds ratios given in the summary to odds ratios.

```{r echo = T, eval = F}
exp(Model1$coefficients)
exp(confint(Model1))
``` 

Use the window below to run the code to reproduce results from the video. 

```{r ex9, exercise = TRUE, exercise.lines = 2}

```

## Exercise 3: Running Logistic Regression with One Predictor

Using the admit data from earlier, run a logistic regression predicting admission (admit( from GRE scores (GRE). Name your model ex10. Be sure to convert to odds ratios and get confidence intervals as well. 

```{r ex10, exercise = TRUE, exercise.lines = 4}

```

```{r ex10-solution}
ex10<-glm(admit_F~gre, data=admit, family = binomial)
summary(ex10)
exp(ex10$coefficients)
exp(confint(ex10))
```

